{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST 데이터 받기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from six.moves import urllib\n",
    "import gzip\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "SOURCE_URL = 'http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz'\n",
    "FILENAME = SOURCE_URL.split('/')[-1]\n",
    "DATA_DIR = './datasets'\n",
    "\n",
    "def maybe_download(data_dir):\n",
    "    filepath = os.path.join(data_dir, FILENAME)\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "    if not os.path.isfile(filepath):\n",
    "        def _progress(count, block_size, total_size):\n",
    "            sys.stdout.write('\\r>> Downloading {} {:.1f} %'.format(\n",
    "                FILENAME, float(count * block_size) / float(total_size) * 100.0))\n",
    "            sys.stdout.flush()\n",
    "        filepath, _ = urllib.request.urlretrieve(SOURCE_URL, filepath, _progress)\n",
    "        print()\n",
    "        statinfo = os.stat(filepath)\n",
    "        print('Successfully donloaded', FILENAME, statinfo.st_size, 'bytes.')\n",
    "\n",
    "def load(data_dir, subset='train'):\n",
    "    maybe_download(data_dir)\n",
    "    filepath = os.path.join(data_dir, FILENAME)\n",
    "    \n",
    "    f = gzip.open(filepath, 'rb')\n",
    "    u = pickle._Unpickler(f)\n",
    "    u.encoding = 'latin1'\n",
    "    train_set, valid_set, test_set = u.load()\n",
    "    f.close()\n",
    "    \n",
    "    if subset == 'train':\n",
    "        trainx, trainy = train_set\n",
    "        trainx = trainx.astype(np.float32).reshape(trainx.shape[0], 28, 28)\n",
    "        trainy = trainy.astype(np.uint8)\n",
    "        return trainx, trainy\n",
    "    elif subset == 'test':\n",
    "        testx, testy = test_set\n",
    "        testx = testx.astype(np.float32).reshape(testx.shape[0], 28, 28)\n",
    "        testy = testy.astype(np.uint8)\n",
    "        return testx, testy\n",
    "    elif subset== 'valid':\n",
    "        validx, validy = valid_set\n",
    "        validx = validx.astype(np.float32).reshape(validx.shape[0], 28, 28)\n",
    "        validy = validy.astype(np.uint8)\n",
    "        return validx, validy\n",
    "    else:\n",
    "        raise NotImplementedError('subset should be train or valid or test')\n",
    "\n",
    "# Load data\n",
    "train_data, train_label = load(DATA_DIR, 'train')\n",
    "valid_data, valid_label = load(DATA_DIR, 'valid')\n",
    "test_data, test_label = load(DATA_DIR, 'test')\n",
    "\n",
    "# concatenate train and valid data as train data\n",
    "train_data = np.concatenate((train_data, valid_data))\n",
    "train_label = np.concatenate((train_label, valid_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST 데이터 확인 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000,)\n",
      "(10000, 28, 28)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "# size of MNIST\n",
    "print(train_data.shape)\n",
    "print(train_label.shape)\n",
    "print(test_data.shape)\n",
    "print(test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show data\n",
    "_, (ax1, ax2) = plt.subplots(1, 2)\n",
    "sample_data = train_data[100]\n",
    "ax1.imshow(sample_data, cmap=plt.cm.Greys);\n",
    "ax2.hist(sample_data, bins=20, range=[0, 1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.nn import rnn_cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "num_epochs = 2\n",
    "batch_size = 100\n",
    "num_display = 100\n",
    "\n",
    "def RNN(X):\n",
    "    X = tf.unstack(X, num=28, axis=1) #쪼개짐\n",
    "    lstm = rnn_cell.LSTMCell(128, forget_bias=0.9) #lstmcell 생성 #forget_gate(스위치 1:켜짐~0:꺼짐)의 크기\n",
    "    outs = tf.nn.static_rnn(lstm, X, dtype=tf.float32)\n",
    "    #print(outs)\n",
    "    #print(len(outs))\n",
    "    outs=outs[0][-1]\n",
    "    return outs\n",
    "    \n",
    "def get_model(X, by):\n",
    "    #X = tf.expand_dims(X, axis=3) # (None, 28, 28, 1)\n",
    "    \n",
    "    outs = RNN(X)\n",
    "    #print(outs)\n",
    "    \n",
    "    #outs = tf.reshape(outs, (-1, outs.shape[1]*outs.shape[2]*outs.shape[3]))\n",
    "    outs = tf.layers.dense(outs, 256)\n",
    "    outs = tf.nn.relu(outs)\n",
    "    #outs=tf.layers.dropout(outs, rate=0.5) #dropout/ 0.5=>매순간마다 50%씩 버리겠다\n",
    "    outs = tf.layers.dense(outs, 10)\n",
    "    one_hot = tf.one_hot(by, 10)\n",
    "    \n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits_v2(logits=outs, \n",
    "                                                      labels=one_hot)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    opt = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "    \n",
    "    preds = tf.cast(tf.argmax(tf.nn.softmax(outs), axis=1), tf.int32)\n",
    "    acc = tf.reduce_mean(tf.cast(tf.equal(by, preds), tf.float32))\n",
    "#     saver = tf.train.Saver(tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, 'second')\n",
    "#                           +tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, 'third')\n",
    "#                           +tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, 'dense')) #미리 학습해놓은 파라미터를 쓰고 싶을 때 # 그래프에서 전역변수 모여있는 것들 다 불러옴 \n",
    "    init = tf.global_variables_initializer()\n",
    "    return {\n",
    "        'loss': loss,\n",
    "        'opt': opt,\n",
    "        'preds': preds,\n",
    "        'acc': acc,\n",
    "        'init': init\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=(None, 28, 28))\n",
    "by = tf.placeholder(tf.int32)\n",
    "\n",
    "model = get_model(X, by)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration 1\n",
      "loss 2.2978 acc 0.1000\n",
      "loss 0.8638 acc 0.6700\n",
      "loss 0.6157 acc 0.8200\n",
      "loss 0.3664 acc 0.8700\n",
      "loss 0.2904 acc 0.8800\n",
      "loss 0.2946 acc 0.9000\n",
      "Current iteration 2\n",
      "loss 0.1612 acc 0.9500\n",
      "loss 0.2965 acc 0.9200\n",
      "loss 0.2602 acc 0.9200\n",
      "loss 0.2086 acc 0.9200\n",
      "loss 0.1048 acc 0.9700\n",
      "loss 0.0744 acc 0.9600\n",
      "TEST: loss 0.1207 acc 0.9631\n"
     ]
    }
   ],
   "source": [
    "#model = get_model(X, by)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(model['init'])\n",
    "    for ind_epoch in range(0, num_epochs):\n",
    "        print('Current iteration {}'.format(ind_epoch + 1))\n",
    "        \n",
    "        for ind_ in range(0, int(60000 / batch_size)):\n",
    "            batch_X = train_data[ind_*batch_size:(ind_+1)*batch_size]\n",
    "            batch_by = train_label[ind_*batch_size:(ind_+1)*batch_size]\n",
    "            _, cur_loss, cur_acc = sess.run(\n",
    "                [model['opt'], model['loss'], model['acc']],\n",
    "                feed_dict={X: batch_X, by: batch_by})\n",
    "            if ind_ % num_display == 0:\n",
    "                print('loss {0:.4f} acc {1:.4f}'.format(cur_loss, cur_acc))\n",
    "    cur_acc_all = 0.0\n",
    "    cur_loss_all = 0.0\n",
    "    for ind_ in range(0, 10):\n",
    "        cur_loss, cur_acc = sess.run(\n",
    "                    [model['loss'], model['acc']],\n",
    "                    feed_dict={X: test_data[ind_*1000:(ind_+1)*1000], \n",
    "                               by: test_label[ind_*1000:(ind_+1)*1000]})\n",
    "        cur_loss_all += cur_loss\n",
    "        cur_acc_all += cur_acc\n",
    "    print('TEST: loss {0:.4f} acc {1:.4f}'.format(cur_loss_all / 10.0, \n",
    "                                                  cur_acc_all / 10.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = get_model(X, by, False) #처음 선언하는 거니깐 False\n",
    "# with tf.Session() as sess:\n",
    "#     sess.run(model['init'])\n",
    "#     for ind_epoch in range(0, num_epochs):\n",
    "#         print('Current iteration {}'.format(ind_epoch + 1))\n",
    "        \n",
    "#         for ind_ in range(0, int(60000 / batch_size)):\n",
    "#             batch_X = train_data[ind_*batch_size:(ind_+1)*batch_size]\n",
    "#             batch_by = train_label[ind_*batch_size:(ind_+1)*batch_size]\n",
    "#             _, cur_loss, cur_acc = sess.run(\n",
    "#                 [model['opt'], model['loss'], model['acc']],\n",
    "#                 feed_dict={X: batch_X, by: batch_by})\n",
    "#             if ind_ % num_display == 0:\n",
    "#                 print('loss {0:.4f} acc {1:.4f}'.format(cur_loss, cur_acc))\n",
    "#     model['saver'].save(sess,'./ours.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = get_model(X, by, True)\n",
    "# with tf.Session() as sess:\n",
    "#     sess.run(model['init']) #first는 저장을 하지 않았기 때문에 걔를 초기화\n",
    "#     model['saver'].restore(sess,'./ours.ckpt')\n",
    "#     cur_acc_all = 0.0\n",
    "#     cur_loss_all = 0.0\n",
    "#     for ind_ in range(0, 10):\n",
    "#         cur_loss, cur_acc = sess.run(\n",
    "#                     [model['loss'], model['acc']],\n",
    "#                     feed_dict={X: test_data[ind_*1000:(ind_+1)*1000], \n",
    "#                                by: test_label[ind_*1000:(ind_+1)*1000]})\n",
    "#         cur_loss_all += cur_loss\n",
    "#         cur_acc_all += cur_acc\n",
    "#     print('TEST: loss {0:.4f} acc {1:.4f}'.format(cur_loss_all / 10.0, \n",
    "#                                                   cur_acc_all / 10.0))\n",
    "\n",
    "# ##first는 학습을 하지 않아서 acc가 좀 낮게 나옴\n",
    "# ##기존에 있던 파라미터를 잘 골라서 저장하고 저장해놓은거를 불러와서 road하고 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
