{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Adversarial Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"GAN.JPG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download MNIST and load it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import shutil\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "from keras.datasets import cifar10\n",
    "(train_data, train_label), (test_data, test_label) = cifar10.load_data()\n",
    "train_data = train_data / 255.\n",
    "test_data = test_data / 255.\n",
    "\n",
    "def img_tile(imgs, aspect_ratio=1.0, tile_shape=None, border=1,\n",
    "             border_color=0):\n",
    "    ''' Tile images in a grid.\n",
    "    If tile_shape is provided only as many images as specified in tile_shape\n",
    "    will be included in the output.\n",
    "    '''\n",
    "\n",
    "    imgs = np.array(imgs)\n",
    "    if imgs.ndim != 3 and imgs.ndim != 4:\n",
    "        raise ValueError('imgs has wrong number of dimensions.')\n",
    "    n_imgs = imgs.shape[0]\n",
    "\n",
    "    # Grid shape\n",
    "    img_shape = np.array(imgs.shape[1:])\n",
    "    if tile_shape is None:\n",
    "        img_aspect_ratio = img_shape[1] / float(img_shape[0])\n",
    "        aspect_ratio *= img_aspect_ratio\n",
    "        tile_height = int(np.ceil(np.sqrt(n_imgs * aspect_ratio)))\n",
    "        tile_width = int(np.ceil(np.sqrt(n_imgs / aspect_ratio)))\n",
    "        grid_shape = np.array((tile_height, tile_width))\n",
    "    else:\n",
    "        assert len(tile_shape) == 2\n",
    "        grid_shape = np.array(tile_shape)\n",
    "\n",
    "    # Tile image shape\n",
    "    tile_img_shape = np.array(imgs.shape[1:])\n",
    "    tile_img_shape[:2] = (img_shape[:2] + border) * grid_shape[:2] - border\n",
    "\n",
    "    # Assemble tile image\n",
    "    tile_img = np.empty(tile_img_shape)\n",
    "    tile_img[:] = border_color\n",
    "    for i in range(grid_shape[0]):\n",
    "        for j in range(grid_shape[1]):\n",
    "            img_idx = j + i * grid_shape[1]\n",
    "            if img_idx >= n_imgs:\n",
    "                # No more images - stop filling out the grid.\n",
    "                break\n",
    "            img = imgs[img_idx]\n",
    "            yoff = (img_shape[0] + border) * i\n",
    "            xoff = (img_shape[1] + border) * j\n",
    "            tile_img[yoff:yoff + img_shape[0], xoff:xoff + img_shape[1], ...] = img\n",
    "\n",
    "    return tile_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## show MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size of MNIST\n",
    "print(train_data.shape)\n",
    "print(train_label.shape)\n",
    "print(test_data.shape)\n",
    "print(test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show data\n",
    "idx = np.random.randint(0, train_data.shape[0])\n",
    "_, (ax1, ax2) = plt.subplots(1, 2)\n",
    "sample_data = train_data[idx]\n",
    "ax1.imshow(sample_data);\n",
    "# ax2.hist(sample_data, bins=20, range=[0, 1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete summary folder and make it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUMMARY_DIR = './gan_summary'\n",
    "TRAIN_DIR = SUMMARY_DIR + '/train'\n",
    "TEST_DIR = SUMMARY_DIR + '/test'\n",
    "IMAGE_DIR = SUMMARY_DIR + '/image'\n",
    "\n",
    "if os.path.exists(SUMMARY_DIR):\n",
    "    shutil.rmtree(SUMMARY_DIR)\n",
    "if not os.path.exists(SUMMARY_DIR):\n",
    "    os.makedirs(SUMMARY_DIR)\n",
    "    os.makedirs(TRAIN_DIR)\n",
    "    os.makedirs(TEST_DIR)\n",
    "    os.makedirs(IMAGE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define tensorflow graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fully_connected(inputs, out_channel, name='fc'):\n",
    "    \"\"\"\n",
    "    very simple fully connected layer function\n",
    "\n",
    "    Args:\n",
    "        inputs: a batch of input tensor [batch_size, n]\n",
    "                where n is the number of input feature dimension\n",
    "        out_channel: output channel dimension\n",
    "\n",
    "    Returns:\n",
    "        fc: inputs * weights + biases [batch_size, out_channel]\n",
    "    \"\"\"\n",
    "    # in_channel: input channel dimension\n",
    "    # w_shape: shape of weight matrix\n",
    "    # b_shape: shape of bias vector\n",
    "    in_channel = inputs.get_shape().as_list()[1]\n",
    "    w_shape = [in_channel, out_channel]\n",
    "    b_shape = [out_channel]\n",
    "\n",
    "    # Define weight matrix variable, bias vector variable\n",
    "    with tf.variable_scope(name):\n",
    "        # To share the variables you have to use\n",
    "        # a function 'tf.get_variable' instead of 'tf.Variable'\n",
    "        weights = tf.get_variable('weights', shape=w_shape,\n",
    "                                  initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "        biases = tf.get_variable('biases', shape=b_shape,\n",
    "                                 initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "        fc = tf.matmul(inputs, weights)\n",
    "        fc = tf.nn.bias_add(fc, biases)\n",
    "\n",
    "        return fc\n",
    "\n",
    "def conv2d(input_, output_dim, k_h=5, k_w=5, d_h=2, d_w=2, stddev=0.02, name=\"conv2d\"):\n",
    "    with tf.variable_scope(name):\n",
    "        w = tf.get_variable('w', [k_h, k_w, input_.get_shape()[-1], output_dim],\n",
    "              initializer=tf.truncated_normal_initializer(stddev=stddev))\n",
    "        conv = tf.nn.conv2d(input_, w, strides=[1, d_h, d_w, 1], padding='SAME')\n",
    "\n",
    "        biases = tf.get_variable('biases', [output_dim], initializer=tf.constant_initializer(0.0))\n",
    "        conv = tf.reshape(tf.nn.bias_add(conv, biases), conv.get_shape())\n",
    "\n",
    "        return conv\n",
    "\n",
    "def bn(x, is_training, scope, axis=-1):\n",
    "    return tf.layers.batch_normalization(x, epsilon = 1e-5, momentum = 0.9, training = is_training, name = scope, axis=axis) # add axis=1\n",
    "\n",
    "def linear(input_, output_size, scope=None, stddev=0.02, bias_start=0.0, with_w=False):\n",
    "    shape = input_.get_shape().as_list()\n",
    "\n",
    "    with tf.variable_scope(scope or \"Linear\"):\n",
    "        matrix = tf.get_variable(\"Matrix\", [shape[1], output_size], tf.float32,\n",
    "                 tf.random_normal_initializer(stddev=stddev))\n",
    "        bias = tf.get_variable(\"bias\", [output_size],\n",
    "        initializer=tf.constant_initializer(bias_start))\n",
    "        if with_w:\n",
    "            return tf.matmul(input_, matrix) + bias, matrix, bias\n",
    "        else:\n",
    "            return tf.matmul(input_, matrix) + bias\n",
    "\n",
    "def deconv2d(input_, output_shape, k_h=5, k_w=5, d_h=2, d_w=2, name=\"deconv2d\", stddev=0.02, with_w=False):\n",
    "    with tf.variable_scope(name):\n",
    "        # filter : [height, width, output_channels, in_channels]\n",
    "        w = tf.get_variable('w', [k_h, k_w, output_shape[-1], input_.get_shape()[-1]],\n",
    "                            initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "\n",
    "        try:\n",
    "            deconv = tf.nn.conv2d_transpose(input_, w, output_shape=output_shape, strides=[1, d_h, d_w, 1])\n",
    "\n",
    "        # Support for verisons of TensorFlow before 0.7.0\n",
    "        except AttributeError:\n",
    "            deconv = tf.nn.deconv2d(input_, w, output_shape=output_shape, strides=[1, d_h, d_w, 1])\n",
    "\n",
    "        biases = tf.get_variable('biases', [output_shape[-1]], initializer=tf.constant_initializer(0.0))\n",
    "        deconv = tf.reshape(tf.nn.bias_add(deconv, biases), deconv.get_shape())\n",
    "\n",
    "        if with_w:\n",
    "            return deconv, w, biases\n",
    "        else:\n",
    "            return deconv\n",
    "def lrelu(x, leak=0.2, name=\"lrelu\"):\n",
    "    return tf.maximum(x, leak*x)\n",
    "        \n",
    "def discriminator(x, reuse=None, is_training=True):\n",
    "    \"\"\"\n",
    "    build the discriminator\n",
    "\n",
    "    Args:\n",
    "        x: a batch of input to the network [batch_size, 32, 32, 3]\n",
    "\n",
    "    returns:\n",
    "        net: output of the discriminator [batch_size, 1]\n",
    "    \"\"\"\n",
    "    with tf.variable_scope('discriminator') as scope:\n",
    "        if reuse:\n",
    "            scope.reuse_variables()\n",
    "\n",
    "#             if self.dataset_name == 'cifar10':\n",
    "        if True:\n",
    "            print(\"D:\",x.get_shape()) # 32, 32, 3 = 3072\n",
    "            net = lrelu(conv2d(x, 64, 5, 5, 2, 2, name='d_conv1'))\n",
    "            print(\"D:\",net.get_shape())\n",
    "            net = lrelu(bn(conv2d(net, 128, 5, 5, 2, 2, name='d_conv2'), is_training=is_training, scope='d_bn2'))\n",
    "            print(\"D:\",net.get_shape())\n",
    "            net = lrelu(bn(conv2d(net, 256, 5, 5, 2, 2, name='d_conv3'), is_training=is_training, scope='d_bn3'))\n",
    "            print(\"D:\",net.get_shape())\n",
    "            net = lrelu(bn(conv2d(net, 512, 5, 5, 2, 2, name='d_conv4'), is_training=is_training, scope='d_bn4'))\n",
    "            print(\"D:\",net.get_shape())\n",
    "            net = tf.reshape(net, [batch_size, -1])\n",
    "            print(\"D:\",net.get_shape())\n",
    "            out_logit = linear(net, 1, scope='d_fc5')\n",
    "            print(\"D:\",net.get_shape())\n",
    "            out = tf.nn.sigmoid(out_logit)\n",
    "            print(\"D:\",out.get_shape())\n",
    "            print(\"------------------------\")\n",
    "\n",
    "        else: # mnist / fashion mnist\n",
    "            #print(x.get_shape())\n",
    "            net = lrelu(conv2d(x, 64, 4, 4, 2, 2, name='d_conv1'))\n",
    "            net = lrelu(bn(conv2d(net, 128, 4, 4, 2, 2, name='d_conv2'), is_training=is_training, scope='d_bn2'))\n",
    "            net = tf.reshape(net, [batch_size, -1])\n",
    "            net = lrelu(bn(linear(net, 1024, scope='d_fc3'), is_training=is_training, scope='d_bn3'))\n",
    "            out_logit = linear(net, 1, scope='d_fc4')\n",
    "            out = tf.nn.sigmoid(out_logit)\n",
    "\n",
    "        return out\n",
    "\n",
    "def generator(z, is_training=True):\n",
    "    \"\"\"\n",
    "    build the generator\n",
    "\n",
    "    Args:\n",
    "        z: a batch of input to the network [batch_size, z_dim]\n",
    "\n",
    "    Returns:\n",
    "        net: output of the generator [batch_size, 28, 28, 1]\n",
    "    \"\"\"\n",
    "    with tf.variable_scope('generator') as scope:\n",
    "\n",
    "        h_size = 32\n",
    "        h_size_2 = 16\n",
    "        h_size_4 = 8\n",
    "        h_size_8 = 4\n",
    "        h_size_16 = 2\n",
    "\n",
    "        print(\"G:\",z.get_shape())\n",
    "        net = linear(z, 512*h_size_16*h_size_16, scope='g_fc1')\n",
    "        print(\"G:\",net.get_shape())\n",
    "        net = tf.nn.relu(\n",
    "            bn(tf.reshape(net, [batch_size, h_size_16, h_size_16, 512]),is_training=is_training, scope='g_bn1')\n",
    "            )\n",
    "        print(\"G:\",net.get_shape())\n",
    "        net = tf.nn.relu(\n",
    "            bn(deconv2d(net, [batch_size, h_size_8, h_size_8, 256], 5, 5, 2, 2, name='g_dc2'),is_training=is_training, scope='g_bn2')\n",
    "            )\n",
    "        print(\"G:\",net.get_shape())\n",
    "        net = tf.nn.relu(\n",
    "            bn(deconv2d(net, [batch_size, h_size_4, h_size_4, 128], 5, 5, 2, 2, name='g_dc3'),is_training=is_training, scope='g_bn3')\n",
    "            )\n",
    "        print(\"G:\",net.get_shape())\n",
    "        net = tf.nn.relu(\n",
    "            bn(deconv2d(net, [batch_size, h_size_2, h_size_2, 64], 5, 5, 2, 2, name='g_dc4'),is_training=is_training, scope='g_bn4')\n",
    "            )\n",
    "        print(\"G:\",net.get_shape())\n",
    "        out = tf.nn.sigmoid(\n",
    "            deconv2d(net, [batch_size, 32, 32, 3], 5, 5, 2, 2, name='g_dc5')\n",
    "            )\n",
    "        print(\"G:\",out.get_shape())\n",
    "        print(\"------------------------\")\n",
    "    return out\n",
    "\n",
    "def get_loss(D_real, D_fake, eps=1e-10):\n",
    "    \"\"\"\n",
    "    get loss of GAN\n",
    "\n",
    "    Args:\n",
    "        D_real: Real Discriminator output [batch_size, 1]\n",
    "        D_fake: Fake discriminator output [batch_size, 1]\n",
    "\n",
    "    Returns:\n",
    "        D_loss: Discriminator loss\n",
    "        G_loss: Generator loss\n",
    "    \"\"\"\n",
    "    D_loss = -(tf.reduce_mean(tf.log(D_real+eps)) + tf.reduce_mean(tf.log(1-D_fake+eps)))\n",
    "    G_loss = -tf.reduce_mean(tf.log(D_fake+eps))\n",
    "\n",
    "    return D_loss, G_loss\n",
    "\n",
    "\n",
    "def get_next_batch(data, label, batch_size):\n",
    "    \"\"\"\n",
    "    get 'batch_size' amount of data and label randomly\n",
    "\n",
    "    Args:\n",
    "        data: data\n",
    "        label: label\n",
    "        batch_size: # of data to get\n",
    "\n",
    "    Returns:\n",
    "        batch_data: data of 'batch_size'\n",
    "        batch_label: coresponding label of batch_data\n",
    "    \"\"\"\n",
    "    n_data = data.shape[0]\n",
    "    random_idx = random.sample(range(1, n_data), batch_size)\n",
    "\n",
    "    batch_data = data[random_idx]\n",
    "    batch_label = label[random_idx]\n",
    "    return batch_data, batch_label\n",
    "\n",
    "\n",
    "# Set hyperparameters\n",
    "batch_size = 100\n",
    "z_dim = 128\n",
    "max_step = 20000\n",
    "lrD = 1e-3\n",
    "lrG = 1e-4\n",
    "beta1 = 0.5\n",
    "\n",
    "############################# Build the model #############################\n",
    "# Define image tensor x placeholder\n",
    "x = __________________________________________________\n",
    "# Define z vector as uniform distribution between [-1, 1]\n",
    "z = __________________________________________________\n",
    "\n",
    "# Build discriminator where input data is real image x\n",
    "______________________________________________________\n",
    "# Build generator\n",
    "______________________________________________________\n",
    "# Build discriminator where input data is generated image G\n",
    "______________________________________________________\n",
    "\n",
    "# Get D_loss and G_loss\n",
    "______________________________________________________\n",
    "\n",
    "# Make optimization op\n",
    "______________________________________________________\n",
    "______________________________________________________\n",
    "\n",
    "# To update the generator and the discriminator\n",
    "# get their network parameters\n",
    "______________________________________________________\n",
    "______________________________________________________\n",
    "\n",
    "# Make train op for each network\n",
    "D_train = ____________________________________________\n",
    "G_train = ____________________________________________\n",
    "\n",
    "# Make initialization op\n",
    "init = tf.global_variables_initializer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    # Define writer\n",
    "    train_writer = tf.summary.FileWriter(TRAIN_DIR, sess.graph)\n",
    "    test_writer = tf.summary.FileWriter(TEST_DIR)\n",
    "\n",
    "    # Initialize variables\n",
    "    sess.run(init)\n",
    "    \n",
    "    # Before train the model, shows train data and save it\n",
    "    batch_x, batch_y = get_next_batch(train_data, train_label, batch_size)\n",
    "    train_tiled = img_tile(batch_x, border_color=1.0)\n",
    "    train_tiled = np.squeeze(train_tiled)\n",
    "    print(\"Training data\")\n",
    "    plt.imshow(train_tiled)\n",
    "    plt.show()\n",
    "    plt.imsave(IMAGE_DIR + '/train.png', train_tiled)\n",
    "    \n",
    "    samples = []\n",
    "    for step in range(max_step):\n",
    "        batch_x, batch_y = get_next_batch(train_data, train_label, batch_size)\n",
    "        \n",
    "        _, d_loss = sess.run([D_train, D_loss], feed_dict={x: batch_x})\n",
    "        _, g_loss = sess.run([G_train, G_loss])\n",
    "        #summary = sess.run(merged, feed_dict={x: batch_x})\n",
    "        #train_writer.add_summary(summary, step)\n",
    "        \n",
    "        # Save generarted data to make gif files\n",
    "        if step % 50 == 0:\n",
    "            g = sess.run(G)\n",
    "            g_tiled = img_tile(g, border_color=1.0)\n",
    "            g_tiled = np.squeeze(g_tiled)\n",
    "            samples.append(g_tiled)\n",
    "        if step % 200 == 0:\n",
    "            print(\"{} steps |  G_loss: {:.4f}, D_loss: {:.4f}\".format(step, g_loss, d_loss))\n",
    "            plt.imshow(g_tiled)\n",
    "            plt.show()\n",
    "            plt.imsave(IMAGE_DIR + '/{}.png'.format(str(step).zfill(6)),\n",
    "                       g_tiled)\n",
    "#             plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "imageio.mimsave(SUMMARY_DIR + '/generated.gif', samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "https://github.com/4thgen/DCGAN-CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
