{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Residual Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR-100 데이터 받기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from six.moves import urllib\n",
    "import gzip\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# from tf.keras.utils import get_file\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.datasets.cifar import load_batch\n",
    "from keras import backend as K\n",
    "\n",
    "def load_data(label_mode='fine'):\n",
    "    if label_mode not in ['fine', 'coarse']:\n",
    "        raise ValueError('`label_mode` must be one of `\"fine\"`, `\"coarse\"`.')\n",
    "\n",
    "    dirname = 'cifar-100-python'\n",
    "    origin = 'https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz'\n",
    "    path = get_file(dirname, origin=origin, untar=True)\n",
    "\n",
    "    fpath = os.path.join(path, 'train')\n",
    "    x_train, y_train = load_batch(fpath, label_key=label_mode + '_labels')\n",
    "\n",
    "    fpath = os.path.join(path, 'test')\n",
    "    x_test, y_test = load_batch(fpath, label_key=label_mode + '_labels')\n",
    "\n",
    "    y_train = np.reshape(y_train, (len(y_train), 1))\n",
    "    y_test = np.reshape(y_test, (len(y_test), 1))\n",
    "\n",
    "    if K.image_data_format() == 'channels_last':\n",
    "        x_train = x_train.transpose(0, 2, 3, 1)\n",
    "        x_test = x_test.transpose(0, 2, 3, 1)\n",
    "\n",
    "    return (x_train, y_train), (x_test, y_test)\n",
    "(train_data,train_label),(test_data,test_label) = load_data()\n",
    "train_data = train_data / 255.0\n",
    "test_data = test_data / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shape 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n",
      "(50000, 1)\n",
      "(10000, 32, 32, 3)\n",
      "(10000, 1)\n"
     ]
    }
   ],
   "source": [
    "# size of cifar-100\n",
    "print(train_data.shape)\n",
    "print(train_label.shape)\n",
    "print(test_data.shape)\n",
    "print(test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show data\n",
    "_, (ax1) = plt.subplots(1)\n",
    "sample_data = train_data[100]\n",
    "ax1.imshow(sample_data, cmap=plt.cm.Greys);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### residual block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_block(X_input, num_filter, chg_dim) :\n",
    "    stride = 1\n",
    "        \n",
    "    #stride=2일 경우\n",
    "    if chg_dim :\n",
    "        stride = 2\n",
    "        pool1 = tf.layers.max_pooling2d(inputs= X_input, strides=2, pool_size=[2,2])\n",
    "        pad1 = tf.pad(pool1, [[0,0], [0,0], [0,0], [int(num_filter/4),int(num_filter/4)]])\n",
    "        shortcut = pad1\n",
    "    else :\n",
    "        shortcut = X_input\n",
    "        \n",
    "    bm1 = tf.layers.batch_normalization(inputs = X_input)\n",
    "    relu1 = tf.nn.relu(bm1)\n",
    "    conv1 = tf.layers.conv2d(inputs = relu1, filters=num_filter, kernel_size=[3, 3], padding=\"SAME\", strides=stride, kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "    bm2 = tf.layers.batch_normalization(inputs = conv1)\n",
    "    relu2 = tf.nn.relu(bm2)\n",
    "    conv2 = tf.layers.conv2d(inputs = relu2, filters=num_filter, kernel_size=[3, 3], padding=\"SAME\", strides=1, kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "        \n",
    "    X_output = conv2 + shortcut\n",
    "        \n",
    "    return X_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### building model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x,by) :\n",
    "    \n",
    "    with tf.variable_scope('first'):\n",
    "        outs = tf.layers.conv2d(inputs = x, filters = 64, kernel_size=[7,7], padding=\"SAME\", strides=2)\n",
    "        outs = tf.layers.batch_normalization(inputs = outs)\n",
    "        outs = tf.nn.relu(outs)\n",
    "    \n",
    "#     #pooling    \n",
    "    outs = tf.layers.max_pooling2d(inputs = outs, strides = 2, pool_size=2)\n",
    "    \n",
    "    \n",
    "    outs = residual_block(outs, 64, False)\n",
    "    outs = residual_block(outs, 64, False)\n",
    "    outs = residual_block(outs, 64, False)\n",
    "\n",
    "    outs = residual_block(outs, 128, True)\n",
    "    outs = residual_block(outs, 128, False)\n",
    "    outs = residual_block(outs, 128, False)\n",
    "    outs = residual_block(outs, 128, False)\n",
    "\n",
    "    outs = residual_block(outs, 256, True)\n",
    "    outs = residual_block(outs, 256, False)\n",
    "    outs = residual_block(outs, 256, False)\n",
    "    outs = residual_block(outs, 256, False)\n",
    "    outs = residual_block(outs, 256, False)\n",
    "    outs = residual_block(outs, 256, False)\n",
    "                                   \n",
    "    outs = residual_block(outs, 512, True)\n",
    "    outs = residual_block(outs, 512, False)\n",
    "    outs = residual_block(outs, 512, False)\n",
    "\n",
    "    #Average Pooling\n",
    "    outs = tf.reduce_mean(outs, [1, 2], keep_dims=True)\n",
    "\n",
    "    outs = tf.reshape(outs, (-1, outs.shape[1]*outs.shape[2]*outs.shape[3]))\n",
    "    outs = tf.layers.dense(outs, 100)\n",
    "                                   \n",
    "    #one_hot\n",
    "    one_hot = tf.squeeze(tf.one_hot(by, 100),axis=1)\n",
    "\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=outs, labels=one_hot))\n",
    "    opt = tf.train.AdamOptimizer(learning_rate=0.0005).minimize(loss)\n",
    "    preds = tf.equal(tf.argmax(outs, 1), tf.argmax(one_hot, 1))     \n",
    "    acc = tf.reduce_mean(tf.cast(preds, tf.float32))\n",
    "    init = tf.global_variables_initializer()\n",
    "                                   \n",
    "    return {\n",
    "        'loss':loss,\n",
    "        'opt':opt,\n",
    "        'preds':preds,\n",
    "        'acc':acc,\n",
    "        'init':init\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-5-69bd761b636d>:33: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From <ipython-input-5-69bd761b636d>:41: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = tf.placeholder(tf.float32, shape=(None, 32, 32, 3))\n",
    "by = tf.placeholder(tf.int32)\n",
    "\n",
    "renset = model(X, by)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training_Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration 1\n",
      "loss 4.7189 acc 0.0200\n",
      "loss 4.2036 acc 0.0600\n",
      "loss 3.9932 acc 0.0600\n",
      "loss 3.8268 acc 0.1600\n",
      "loss 3.5803 acc 0.1700\n",
      "Current iteration 2\n",
      "loss 3.3569 acc 0.2300\n",
      "loss 3.2013 acc 0.1900\n",
      "loss 3.0689 acc 0.2500\n",
      "loss 3.1594 acc 0.2400\n",
      "loss 3.0594 acc 0.2500\n",
      "Current iteration 3\n",
      "loss 2.9155 acc 0.3100\n",
      "loss 2.7386 acc 0.2500\n",
      "loss 2.4752 acc 0.3800\n",
      "loss 2.6805 acc 0.3100\n",
      "loss 2.7920 acc 0.2600\n",
      "Current iteration 4\n",
      "loss 2.5713 acc 0.3200\n",
      "loss 2.3051 acc 0.3500\n",
      "loss 2.1145 acc 0.4000\n",
      "loss 2.3277 acc 0.3800\n",
      "loss 2.3418 acc 0.3300\n",
      "Current iteration 5\n",
      "loss 2.1869 acc 0.4000\n",
      "loss 2.0423 acc 0.4600\n",
      "loss 1.6938 acc 0.5300\n",
      "loss 1.9872 acc 0.4900\n",
      "loss 1.9415 acc 0.4700\n",
      "Current iteration 6\n",
      "loss 1.8125 acc 0.5200\n",
      "loss 1.7999 acc 0.4700\n",
      "loss 1.3708 acc 0.6300\n",
      "loss 1.4912 acc 0.6100\n",
      "loss 1.6806 acc 0.5000\n",
      "Current iteration 7\n",
      "loss 1.2912 acc 0.6400\n",
      "loss 1.2784 acc 0.6200\n",
      "loss 1.1696 acc 0.6800\n",
      "loss 1.4182 acc 0.6300\n",
      "loss 1.3492 acc 0.6200\n",
      "Current iteration 8\n",
      "loss 0.9878 acc 0.6500\n",
      "loss 0.9748 acc 0.7200\n",
      "loss 0.8440 acc 0.7600\n",
      "loss 1.1480 acc 0.7000\n",
      "loss 1.1576 acc 0.6700\n",
      "Current iteration 9\n",
      "loss 0.9863 acc 0.6800\n",
      "loss 0.7639 acc 0.7600\n",
      "loss 0.5243 acc 0.8400\n",
      "loss 0.4791 acc 0.8400\n",
      "loss 0.7486 acc 0.8000\n",
      "Current iteration 10\n",
      "loss 0.4857 acc 0.8600\n",
      "loss 0.5564 acc 0.8000\n",
      "loss 0.3853 acc 0.8800\n",
      "loss 0.5938 acc 0.7900\n",
      "loss 0.5397 acc 0.8000\n",
      "Current iteration 11\n",
      "loss 0.6384 acc 0.8200\n",
      "loss 0.4165 acc 0.8600\n",
      "loss 0.5571 acc 0.8600\n",
      "loss 0.2207 acc 0.9500\n",
      "loss 0.4409 acc 0.9100\n",
      "Current iteration 12\n",
      "loss 0.3642 acc 0.9000\n",
      "loss 0.3659 acc 0.8900\n",
      "loss 0.2939 acc 0.9000\n",
      "loss 0.4886 acc 0.8300\n",
      "loss 0.5164 acc 0.8600\n",
      "Current iteration 13\n",
      "loss 0.2090 acc 0.9100\n",
      "loss 0.3449 acc 0.8600\n",
      "loss 0.2042 acc 0.9300\n",
      "loss 0.2560 acc 0.9300\n",
      "loss 0.2572 acc 0.9100\n",
      "Current iteration 14\n",
      "loss 0.1410 acc 0.9600\n",
      "loss 0.1701 acc 0.9400\n",
      "loss 0.2086 acc 0.9200\n",
      "loss 0.1911 acc 0.9300\n",
      "loss 0.1945 acc 0.9300\n",
      "Current iteration 15\n",
      "loss 0.2615 acc 0.9300\n",
      "loss 0.1225 acc 0.9400\n",
      "loss 0.1718 acc 0.9400\n",
      "loss 0.1913 acc 0.9400\n",
      "loss 0.3574 acc 0.9400\n",
      "Current iteration 16\n",
      "loss 0.2052 acc 0.9300\n",
      "loss 0.1042 acc 0.9800\n",
      "loss 0.1035 acc 0.9800\n",
      "loss 0.0931 acc 0.9600\n",
      "loss 0.1539 acc 0.9400\n",
      "Current iteration 17\n",
      "loss 0.0666 acc 0.9900\n",
      "loss 0.1240 acc 0.9700\n",
      "loss 0.1466 acc 0.9500\n",
      "loss 0.1857 acc 0.9200\n",
      "loss 0.1468 acc 0.9400\n",
      "Current iteration 18\n",
      "loss 0.1996 acc 0.9500\n",
      "loss 0.1682 acc 0.9400\n",
      "loss 0.1572 acc 0.9300\n",
      "loss 0.0716 acc 0.9900\n",
      "loss 0.1643 acc 0.9500\n",
      "Current iteration 19\n",
      "loss 0.1773 acc 0.9400\n",
      "loss 0.1207 acc 0.9600\n",
      "loss 0.1165 acc 0.9600\n",
      "loss 0.2902 acc 0.9200\n",
      "loss 0.2497 acc 0.9100\n",
      "Current iteration 20\n",
      "loss 0.1412 acc 0.9600\n",
      "loss 0.1489 acc 0.9500\n",
      "loss 0.1972 acc 0.9600\n",
      "loss 0.2665 acc 0.9100\n",
      "loss 0.1349 acc 0.9300\n",
      "Current iteration 21\n",
      "loss 0.2188 acc 0.9400\n",
      "loss 0.0445 acc 0.9800\n",
      "loss 0.2690 acc 0.9300\n",
      "loss 0.1752 acc 0.9400\n",
      "loss 0.1494 acc 0.9600\n",
      "Current iteration 22\n",
      "loss 0.0854 acc 0.9900\n",
      "loss 0.1522 acc 0.9800\n",
      "loss 0.0305 acc 1.0000\n",
      "loss 0.0277 acc 0.9900\n",
      "loss 0.1857 acc 0.9400\n",
      "Current iteration 23\n",
      "loss 0.1859 acc 0.9400\n",
      "loss 0.1482 acc 0.9500\n",
      "loss 0.1447 acc 0.9700\n",
      "loss 0.0815 acc 0.9800\n",
      "loss 0.0875 acc 0.9700\n",
      "Current iteration 24\n",
      "loss 0.1438 acc 0.9500\n",
      "loss 0.1483 acc 0.9400\n",
      "loss 0.1185 acc 0.9700\n",
      "loss 0.2289 acc 0.9300\n",
      "loss 0.3590 acc 0.9400\n",
      "Current iteration 25\n",
      "loss 0.2031 acc 0.9300\n",
      "loss 0.1535 acc 0.9600\n",
      "loss 0.1329 acc 0.9600\n",
      "loss 0.1505 acc 0.9500\n",
      "loss 0.1149 acc 0.9500\n",
      "Current iteration 26\n",
      "loss 0.0508 acc 0.9800\n",
      "loss 0.1914 acc 0.9300\n",
      "loss 0.2476 acc 0.9500\n",
      "loss 0.1117 acc 0.9700\n",
      "loss 0.1276 acc 0.9600\n",
      "Current iteration 27\n",
      "loss 0.1445 acc 0.9700\n",
      "loss 0.1279 acc 0.9700\n",
      "loss 0.1102 acc 0.9700\n",
      "loss 0.0998 acc 0.9800\n",
      "loss 0.0656 acc 0.9800\n",
      "Current iteration 28\n",
      "loss 0.2045 acc 0.9500\n",
      "loss 0.0724 acc 0.9800\n",
      "loss 0.1299 acc 0.9600\n",
      "loss 0.0577 acc 0.9700\n",
      "loss 0.0711 acc 0.9600\n",
      "Current iteration 29\n",
      "loss 0.0753 acc 0.9800\n",
      "loss 0.1680 acc 0.9400\n",
      "loss 0.1252 acc 0.9600\n",
      "loss 0.0604 acc 0.9700\n",
      "loss 0.1152 acc 0.9600\n",
      "Current iteration 30\n",
      "loss 0.0664 acc 0.9900\n",
      "loss 0.0992 acc 0.9600\n",
      "loss 0.0511 acc 0.9900\n",
      "loss 0.0552 acc 0.9900\n",
      "loss 0.2214 acc 0.9400\n",
      "Current iteration 31\n",
      "loss 0.0735 acc 0.9900\n",
      "loss 0.0484 acc 0.9900\n",
      "loss 0.0932 acc 0.9700\n",
      "loss 0.1434 acc 0.9700\n",
      "loss 0.2473 acc 0.9400\n",
      "Current iteration 32\n",
      "loss 0.0812 acc 0.9600\n",
      "loss 0.0850 acc 0.9700\n",
      "loss 0.1033 acc 0.9600\n",
      "loss 0.1240 acc 0.9800\n",
      "loss 0.2578 acc 0.9800\n",
      "Current iteration 33\n",
      "loss 0.2035 acc 0.9200\n",
      "loss 0.2745 acc 0.9300\n",
      "loss 0.1231 acc 0.9700\n",
      "loss 0.0474 acc 1.0000\n",
      "loss 0.0595 acc 0.9900\n",
      "Current iteration 34\n",
      "loss 0.0481 acc 0.9800\n",
      "loss 0.0510 acc 0.9800\n",
      "loss 0.1777 acc 0.9400\n",
      "loss 0.1017 acc 0.9800\n",
      "loss 0.1134 acc 0.9700\n",
      "Current iteration 35\n",
      "loss 0.0428 acc 0.9900\n",
      "loss 0.0796 acc 0.9700\n",
      "loss 0.0256 acc 1.0000\n",
      "loss 0.1131 acc 0.9800\n",
      "loss 0.1948 acc 0.9300\n",
      "Current iteration 36\n",
      "loss 0.0907 acc 0.9800\n",
      "loss 0.1612 acc 0.9500\n",
      "loss 0.1946 acc 0.9600\n",
      "loss 0.0385 acc 0.9900\n",
      "loss 0.1123 acc 0.9600\n",
      "Current iteration 37\n",
      "loss 0.0336 acc 1.0000\n",
      "loss 0.0796 acc 0.9800\n",
      "loss 0.0473 acc 0.9900\n",
      "loss 0.0492 acc 0.9800\n",
      "loss 0.0991 acc 0.9700\n",
      "Current iteration 38\n",
      "loss 0.0938 acc 0.9700\n",
      "loss 0.0373 acc 0.9900\n",
      "loss 0.0828 acc 0.9800\n",
      "loss 0.1143 acc 0.9600\n",
      "loss 0.2111 acc 0.9400\n",
      "Current iteration 39\n",
      "loss 0.0935 acc 0.9600\n",
      "loss 0.0237 acc 1.0000\n",
      "loss 0.0121 acc 1.0000\n",
      "loss 0.1561 acc 0.9500\n",
      "loss 0.1474 acc 0.9500\n",
      "Current iteration 40\n",
      "loss 0.1538 acc 0.9500\n",
      "loss 0.0673 acc 0.9800\n",
      "loss 0.0563 acc 0.9800\n",
      "loss 0.0272 acc 1.0000\n",
      "loss 0.1325 acc 0.9700\n",
      "Current iteration 41\n",
      "loss 0.1119 acc 0.9600\n",
      "loss 0.1379 acc 0.9500\n",
      "loss 0.0796 acc 0.9600\n",
      "loss 0.1436 acc 0.9500\n",
      "loss 0.0933 acc 0.9600\n",
      "Current iteration 42\n",
      "loss 0.1066 acc 0.9700\n",
      "loss 0.0342 acc 1.0000\n",
      "loss 0.0357 acc 0.9800\n",
      "loss 0.1218 acc 0.9600\n",
      "loss 0.0673 acc 0.9900\n",
      "Current iteration 43\n",
      "loss 0.0919 acc 0.9800\n",
      "loss 0.0401 acc 0.9800\n",
      "loss 0.0466 acc 0.9800\n",
      "loss 0.1328 acc 0.9700\n",
      "loss 0.1103 acc 0.9700\n",
      "Current iteration 44\n",
      "loss 0.0730 acc 0.9800\n",
      "loss 0.0434 acc 0.9900\n",
      "loss 0.0229 acc 1.0000\n",
      "loss 0.0462 acc 0.9900\n",
      "loss 0.1360 acc 0.9600\n",
      "Current iteration 45\n",
      "loss 0.0942 acc 0.9900\n",
      "loss 0.1147 acc 0.9700\n",
      "loss 0.0706 acc 0.9800\n",
      "loss 0.1193 acc 0.9800\n",
      "loss 0.0518 acc 0.9700\n",
      "Current iteration 46\n",
      "loss 0.0510 acc 0.9800\n",
      "loss 0.0158 acc 1.0000\n",
      "loss 0.0276 acc 0.9900\n",
      "loss 0.0727 acc 0.9800\n",
      "loss 0.2870 acc 0.9700\n",
      "Current iteration 47\n",
      "loss 0.1377 acc 0.9500\n",
      "loss 0.1378 acc 0.9800\n",
      "loss 0.1658 acc 0.9700\n",
      "loss 0.0538 acc 0.9800\n",
      "loss 0.0943 acc 0.9600\n",
      "Current iteration 48\n",
      "loss 0.2310 acc 0.9600\n",
      "loss 0.0310 acc 0.9900\n",
      "loss 0.1335 acc 0.9700\n",
      "loss 0.0942 acc 0.9800\n",
      "loss 0.0929 acc 0.9600\n",
      "Current iteration 49\n",
      "loss 0.0823 acc 0.9800\n",
      "loss 0.1320 acc 0.9500\n",
      "loss 0.0205 acc 1.0000\n",
      "loss 0.0396 acc 0.9900\n",
      "loss 0.0912 acc 0.9600\n",
      "Current iteration 50\n",
      "loss 0.1172 acc 0.9600\n",
      "loss 0.1830 acc 0.9600\n",
      "loss 0.0667 acc 0.9700\n",
      "loss 0.0467 acc 0.9900\n",
      "loss 0.0370 acc 0.9900\n",
      "Current iteration 51\n",
      "loss 0.0301 acc 0.9900\n",
      "loss 0.1414 acc 0.9600\n",
      "loss 0.0213 acc 0.9900\n",
      "loss 0.0608 acc 0.9800\n",
      "loss 0.0503 acc 0.9900\n",
      "Current iteration 52\n",
      "loss 0.0260 acc 1.0000\n",
      "loss 0.1143 acc 0.9700\n",
      "loss 0.1511 acc 0.9600\n",
      "loss 0.1625 acc 0.9500\n",
      "loss 0.1284 acc 0.9700\n",
      "Current iteration 53\n",
      "loss 0.0557 acc 0.9700\n",
      "loss 0.1339 acc 0.9600\n",
      "loss 0.0870 acc 0.9800\n",
      "loss 0.0408 acc 0.9800\n",
      "loss 0.0255 acc 0.9900\n",
      "Current iteration 54\n",
      "loss 0.0348 acc 1.0000\n",
      "loss 0.0362 acc 0.9900\n",
      "loss 0.0306 acc 0.9900\n",
      "loss 0.1025 acc 0.9800\n",
      "loss 0.0949 acc 0.9600\n",
      "Current iteration 55\n",
      "loss 0.1595 acc 0.9500\n",
      "loss 0.0423 acc 0.9900\n",
      "loss 0.2753 acc 0.9200\n",
      "loss 0.0805 acc 0.9700\n",
      "loss 0.0580 acc 0.9800\n",
      "Current iteration 56\n",
      "loss 0.0317 acc 0.9900\n",
      "loss 0.1484 acc 0.9700\n",
      "loss 0.0083 acc 1.0000\n",
      "loss 0.0797 acc 0.9800\n",
      "loss 0.1119 acc 0.9900\n",
      "Current iteration 57\n",
      "loss 0.1193 acc 0.9700\n",
      "loss 0.0379 acc 0.9800\n",
      "loss 0.0870 acc 0.9600\n",
      "loss 0.0714 acc 0.9900\n",
      "loss 0.0662 acc 0.9700\n",
      "Current iteration 58\n",
      "loss 0.2158 acc 0.9300\n",
      "loss 0.0239 acc 0.9900\n",
      "loss 0.1216 acc 0.9500\n",
      "loss 0.0927 acc 0.9700\n",
      "loss 0.0290 acc 1.0000\n",
      "Current iteration 59\n",
      "loss 0.0924 acc 0.9600\n",
      "loss 0.2125 acc 0.9500\n",
      "loss 0.0539 acc 0.9800\n",
      "loss 0.0551 acc 0.9600\n",
      "loss 0.0558 acc 0.9700\n",
      "Current iteration 60\n",
      "loss 0.0225 acc 0.9900\n",
      "loss 0.0090 acc 1.0000\n",
      "loss 0.0360 acc 0.9900\n",
      "loss 0.1723 acc 0.9800\n",
      "loss 0.0492 acc 0.9800\n",
      "Current iteration 61\n",
      "loss 0.1258 acc 0.9600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.0103 acc 1.0000\n",
      "loss 0.0554 acc 0.9800\n",
      "loss 0.1263 acc 0.9800\n",
      "loss 0.1193 acc 0.9800\n",
      "Current iteration 62\n",
      "loss 0.0201 acc 0.9900\n",
      "loss 0.1086 acc 0.9700\n",
      "loss 0.0291 acc 0.9900\n",
      "loss 0.0300 acc 0.9900\n",
      "loss 0.1515 acc 0.9500\n",
      "Current iteration 63\n",
      "loss 0.0777 acc 0.9800\n",
      "loss 0.0221 acc 0.9900\n",
      "loss 0.1458 acc 0.9700\n",
      "loss 0.0796 acc 0.9800\n",
      "loss 0.0702 acc 0.9900\n",
      "Current iteration 64\n",
      "loss 0.0268 acc 1.0000\n",
      "loss 0.0410 acc 0.9800\n",
      "loss 0.0421 acc 0.9900\n",
      "loss 0.0386 acc 0.9900\n",
      "loss 0.0613 acc 0.9900\n",
      "Current iteration 65\n",
      "loss 0.0606 acc 0.9800\n",
      "loss 0.0806 acc 0.9700\n",
      "loss 0.0120 acc 1.0000\n",
      "loss 0.1127 acc 0.9800\n",
      "loss 0.0916 acc 0.9800\n",
      "Current iteration 66\n",
      "loss 0.0296 acc 0.9900\n",
      "loss 0.1834 acc 0.9600\n",
      "loss 0.0264 acc 0.9800\n",
      "loss 0.0965 acc 0.9700\n",
      "loss 0.0246 acc 0.9900\n",
      "Current iteration 67\n",
      "loss 0.1695 acc 0.9600\n",
      "loss 0.0165 acc 1.0000\n",
      "loss 0.0198 acc 0.9900\n",
      "loss 0.0106 acc 1.0000\n",
      "loss 0.0196 acc 1.0000\n",
      "Current iteration 68\n",
      "loss 0.1057 acc 0.9700\n",
      "loss 0.1381 acc 0.9600\n",
      "loss 0.0262 acc 0.9900\n",
      "loss 0.0265 acc 0.9900\n",
      "loss 0.0786 acc 0.9600\n",
      "Current iteration 69\n",
      "loss 0.0145 acc 1.0000\n",
      "loss 0.0290 acc 0.9800\n",
      "loss 0.0163 acc 1.0000\n",
      "loss 0.0446 acc 0.9800\n",
      "loss 0.0290 acc 0.9900\n",
      "Current iteration 70\n",
      "loss 0.0291 acc 0.9900\n",
      "loss 0.0387 acc 0.9900\n",
      "loss 0.0238 acc 0.9900\n",
      "loss 0.0289 acc 0.9800\n",
      "loss 0.0887 acc 0.9700\n",
      "Current iteration 71\n",
      "loss 0.0388 acc 0.9900\n",
      "loss 0.0390 acc 0.9800\n",
      "loss 0.0315 acc 0.9800\n",
      "loss 0.0435 acc 0.9800\n",
      "loss 0.1020 acc 0.9600\n",
      "Current iteration 72\n",
      "loss 0.1300 acc 0.9700\n",
      "loss 0.0298 acc 1.0000\n",
      "loss 0.0640 acc 0.9800\n",
      "loss 0.0529 acc 0.9700\n",
      "loss 0.0607 acc 0.9900\n",
      "Current iteration 73\n",
      "loss 0.0194 acc 1.0000\n",
      "loss 0.1005 acc 0.9700\n",
      "loss 0.0293 acc 0.9800\n",
      "loss 0.1080 acc 0.9700\n",
      "loss 0.0238 acc 1.0000\n",
      "Current iteration 74\n",
      "loss 0.2003 acc 0.9300\n",
      "loss 0.0964 acc 0.9700\n",
      "loss 0.0459 acc 0.9900\n",
      "loss 0.0200 acc 1.0000\n",
      "loss 0.0716 acc 0.9800\n",
      "Current iteration 75\n",
      "loss 0.0531 acc 0.9800\n",
      "loss 0.0679 acc 0.9800\n",
      "loss 0.0831 acc 0.9700\n",
      "loss 0.0292 acc 0.9900\n",
      "loss 0.0589 acc 0.9800\n",
      "Current iteration 76\n",
      "loss 0.0461 acc 0.9800\n",
      "loss 0.0117 acc 1.0000\n",
      "loss 0.1269 acc 0.9400\n",
      "loss 0.1096 acc 0.9700\n",
      "loss 0.0762 acc 0.9800\n",
      "Current iteration 77\n",
      "loss 0.0819 acc 0.9700\n",
      "loss 0.0841 acc 0.9800\n",
      "loss 0.0210 acc 1.0000\n",
      "loss 0.0542 acc 0.9800\n",
      "loss 0.0717 acc 0.9700\n",
      "Current iteration 78\n",
      "loss 0.0419 acc 0.9900\n",
      "loss 0.0561 acc 0.9800\n",
      "loss 0.0646 acc 0.9800\n",
      "loss 0.0393 acc 0.9800\n",
      "loss 0.0129 acc 1.0000\n",
      "Current iteration 79\n",
      "loss 0.0041 acc 1.0000\n",
      "loss 0.0343 acc 0.9900\n",
      "loss 0.0362 acc 0.9800\n",
      "loss 0.0770 acc 0.9800\n",
      "loss 0.0523 acc 0.9800\n",
      "Current iteration 80\n",
      "loss 0.1117 acc 0.9500\n",
      "loss 0.0478 acc 0.9900\n",
      "loss 0.0631 acc 0.9900\n",
      "loss 0.0364 acc 0.9800\n",
      "loss 0.0111 acc 1.0000\n",
      "Current iteration 81\n",
      "loss 0.0386 acc 0.9800\n",
      "loss 0.1113 acc 0.9700\n",
      "loss 0.0910 acc 0.9600\n",
      "loss 0.0518 acc 0.9800\n",
      "loss 0.0573 acc 0.9700\n",
      "Current iteration 82\n",
      "loss 0.0338 acc 0.9900\n",
      "loss 0.0779 acc 0.9800\n",
      "loss 0.0214 acc 1.0000\n",
      "loss 0.0254 acc 0.9900\n",
      "loss 0.0301 acc 0.9900\n",
      "Current iteration 83\n",
      "loss 0.1142 acc 0.9700\n",
      "loss 0.0559 acc 0.9800\n",
      "loss 0.0307 acc 0.9900\n",
      "loss 0.1008 acc 0.9700\n",
      "loss 0.0250 acc 0.9900\n",
      "Current iteration 84\n",
      "loss 0.0112 acc 0.9900\n",
      "loss 0.0467 acc 0.9900\n",
      "loss 0.0266 acc 0.9900\n",
      "loss 0.1265 acc 0.9500\n",
      "loss 0.0899 acc 0.9600\n",
      "Current iteration 85\n",
      "loss 0.0429 acc 0.9800\n",
      "loss 0.0334 acc 0.9900\n",
      "loss 0.0643 acc 0.9900\n",
      "loss 0.0620 acc 0.9900\n",
      "loss 0.0581 acc 0.9800\n",
      "Current iteration 86\n",
      "loss 0.0953 acc 0.9700\n",
      "loss 0.0173 acc 0.9900\n",
      "loss 0.0639 acc 0.9800\n",
      "loss 0.0163 acc 1.0000\n",
      "loss 0.0220 acc 0.9900\n",
      "Current iteration 87\n",
      "loss 0.0219 acc 0.9800\n",
      "loss 0.0728 acc 0.9600\n",
      "loss 0.0051 acc 1.0000\n",
      "loss 0.0145 acc 1.0000\n",
      "loss 0.0963 acc 0.9800\n",
      "Current iteration 88\n",
      "loss 0.0405 acc 0.9900\n",
      "loss 0.0147 acc 1.0000\n",
      "loss 0.0356 acc 0.9800\n",
      "loss 0.0387 acc 0.9900\n",
      "loss 0.0362 acc 0.9900\n",
      "Current iteration 89\n",
      "loss 0.0431 acc 0.9800\n",
      "loss 0.0305 acc 0.9800\n",
      "loss 0.0599 acc 0.9800\n",
      "loss 0.0135 acc 1.0000\n",
      "loss 0.0799 acc 0.9800\n",
      "Current iteration 90\n",
      "loss 0.0657 acc 0.9800\n",
      "loss 0.1600 acc 0.9600\n",
      "loss 0.0120 acc 1.0000\n",
      "loss 0.0356 acc 0.9900\n",
      "loss 0.0824 acc 0.9600\n",
      "Current iteration 91\n",
      "loss 0.0621 acc 0.9900\n",
      "loss 0.0421 acc 0.9900\n",
      "loss 0.0602 acc 0.9700\n",
      "loss 0.0442 acc 0.9700\n",
      "loss 0.0467 acc 0.9800\n",
      "Current iteration 92\n",
      "loss 0.0061 acc 1.0000\n",
      "loss 0.0247 acc 0.9900\n",
      "loss 0.0848 acc 0.9500\n",
      "loss 0.0980 acc 0.9900\n",
      "loss 0.0649 acc 0.9700\n",
      "Current iteration 93\n",
      "loss 0.0377 acc 0.9900\n",
      "loss 0.0586 acc 0.9800\n",
      "loss 0.1145 acc 0.9600\n",
      "loss 0.0549 acc 0.9900\n",
      "loss 0.0258 acc 0.9800\n",
      "Current iteration 94\n",
      "loss 0.0044 acc 1.0000\n",
      "loss 0.0336 acc 0.9900\n",
      "loss 0.0480 acc 0.9900\n",
      "loss 0.0652 acc 0.9700\n",
      "loss 0.0691 acc 0.9900\n",
      "Current iteration 95\n",
      "loss 0.0471 acc 0.9900\n",
      "loss 0.0404 acc 0.9900\n",
      "loss 0.1084 acc 0.9800\n",
      "loss 0.0547 acc 0.9800\n",
      "loss 0.0917 acc 0.9800\n",
      "Current iteration 96\n",
      "loss 0.0106 acc 1.0000\n",
      "loss 0.0095 acc 1.0000\n",
      "loss 0.0071 acc 1.0000\n",
      "loss 0.1921 acc 0.9600\n",
      "loss 0.0231 acc 0.9900\n",
      "Current iteration 97\n",
      "loss 0.0375 acc 0.9800\n",
      "loss 0.0131 acc 1.0000\n",
      "loss 0.0013 acc 1.0000\n",
      "loss 0.0182 acc 0.9900\n",
      "loss 0.0474 acc 0.9800\n",
      "Current iteration 98\n",
      "loss 0.0289 acc 0.9900\n",
      "loss 0.0195 acc 0.9900\n",
      "loss 0.0165 acc 1.0000\n",
      "loss 0.0320 acc 0.9800\n",
      "loss 0.1207 acc 0.9900\n",
      "Current iteration 99\n",
      "loss 0.0935 acc 0.9700\n",
      "loss 0.0341 acc 0.9800\n",
      "loss 0.0372 acc 0.9900\n",
      "loss 0.0162 acc 1.0000\n",
      "loss 0.0463 acc 0.9800\n",
      "Current iteration 100\n",
      "loss 0.0363 acc 0.9800\n",
      "loss 0.0617 acc 0.9700\n",
      "loss 0.0352 acc 0.9900\n",
      "loss 0.0462 acc 0.9900\n",
      "loss 0.1654 acc 0.9800\n",
      "TEST: loss 7.0586 acc 0.3230\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "batch_size = 100\n",
    "num_display = 100\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(renset['init'])\n",
    "    for ind_epoch in range(0, num_epochs):\n",
    "        print('Current iteration {}'.format(ind_epoch + 1))\n",
    "\n",
    "        for ind_ in range(0, int(50000 / batch_size)):\n",
    "            batch_X = train_data[ind_*batch_size:(ind_+1)*batch_size]\n",
    "            batch_by = train_label[ind_*batch_size:(ind_+1)*batch_size]\n",
    "            _, cur_loss, cur_acc = sess.run(\n",
    "                [renset['opt'], renset['loss'], renset['acc']],\n",
    "                feed_dict={X: batch_X, by: batch_by})\n",
    "            if ind_ % num_display == 0:\n",
    "                print('loss {0:.4f} acc {1:.4f}'.format(cur_loss, cur_acc))\n",
    "    cur_acc_all = 0.0\n",
    "    cur_loss_all = 0.0\n",
    "    for ind_ in range(0, 10):\n",
    "        cur_loss, cur_acc = sess.run(\n",
    "                    [renset['loss'], renset['acc']],\n",
    "                    feed_dict={X: test_data[ind_*1000:(ind_+1)*1000], \n",
    "                               by: test_label[ind_*1000:(ind_+1)*1000]})\n",
    "        cur_loss_all += cur_loss\n",
    "        cur_acc_all += cur_acc\n",
    "    print('TEST: loss {0:.4f} acc {1:.4f}'.format(cur_loss_all / 10.0, \n",
    "                                                  cur_acc_all / 10.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
